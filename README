Install Hadoop-2.7.1 on Ubuntu 15.10

官网提供了下载，有源码和编译好的版本，但是编译好的版本中native库是32位的，所以64为系统要自己下载源码编译。
参看这里：http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/NativeLibraries.html#Native_Shared_Libraries

一、安装环境
下载好的源码中有BUILDING.txt，按照里面的要求搭好环境：

1.安装jdk 推荐的是1.7，我安装的是
java version "1.8.0_71"
Java(TM) SE Runtime Environment (build 1.8.0_71-b15)
Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode)
去官网下载jdk解压后放到方便的位置，java每个用户都有用，所以我放到/usr/lib/java/jdk1.8.0_71.
编辑/etc/environment:
JAVA_HOME="/usr/lib/java/jdk1.8.0_71"
CLASSPATH=".:/usr/lib/java/jdk1.8.0_71/lib"
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/java/jdk1.8.0_71/bin"
设置默认jdk:
sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk1.7.0_67/bin/java 300  
sudo update-alternatives --install /usr/bin/javac javac /usr/lib/jvm/jdk1.7.0_67/bin/javac 300  
测试一下：java -version

2.安装maven。BUILDING.txt里面直接apt-get，但是不同的时候得到的版本不一样，编译过程会提示错误，要求的版本是3.3.9，所以还是自己下方便：
http://maven.apache.org/download.cgi?Preferred=http%3A%2F%2Fmirrors.cnnic.cn%2Fapache%2F#
安装简单，解压然后加到PATH中就行了：
export MAVEN_INSTALL=/home/ryan/apache-maven-3.3.9
export PATH=$PATH:$HADOOP_INSTALL/bin:$MAVEN_INSTALL/bin
http://maven.apache.org/install.html#
mvn -v 检查一下：
Apache Maven 3.3.9 (bb52d8502b132ec0a5a3f4c09453c07478323dc5; 2015-11-11T00:41:47+08:00)
Maven home: /home/ryan/apache-maven-3.3.9
Java version: 1.8.0_71, vendor: Oracle Corporation
Java home: /usr/lib/java/jdk1.8.0_71/jre
Default locale: zh_CN, platform encoding: UTF-8
OS name: "linux", version: "4.2.0-25-generic", arch: "amd64", family: "unix"

3.安装库：
  $ sudo apt-get -y install build-essential autoconf automake libtool cmake zlib1g-dev pkg-config libssl-dev

4.安装ProtocolBuffer 2.5.0，必须是这个版本：
下载源码包，解压后./configure;make install
默认安装在/usr/local/lib下，添加到LD_LIBRARY_PATH中：
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib

二、安装hadoop

1.编译hadoop.在源码的根目录下执行
mvn package -Pdist,native -DskipTests -Dtar
编译出来的hadoop在这里hadoop-dist/target/hadoop-2.7.1

2.确保$JAVA_HOME正确：
echo $JAVA_HOME
/usr/lib/java/jdk1.8.0_71
然后修改环境:
export HADOOP_INSTALL=/home/ryan/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1
export PATH=$PATH:$HADOOP_INSTALL/bin:$MAVEN_INSTALL/bin
测试一下:
$ hadoop version
Hadoop 2.7.1
Subversion Unknown -r Unknown
Compiled by ryan on 2016-01-22T09:35Z
Compiled with protoc 2.5.0
From source with checksum fc0a1a23fc1868e4d5ee7fa2b28a58a
This command was run using /home/ryan/hadoop-2.7.1-src/hadoop-dist/target/hadoop-2.7.1/share/hadoop/common/hadoop-common-2.7.1.jar
如果使用Psuedo/Fully-Distributed模式，还要保证有ssh.

3.三种模式
参考这里：http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation
Standalone:默认就是这种模式。
Pseudo-Distributed:伪随机模式。我们用这个模式，因为后面c++程序+pipes不能用Standalone模式。
Fully-Distributed:工程用这中模式，我们测试就用上面两种就够了。
配置Pseudo-Distributed模式：
a.配置ssh，要保证可以不输入密码登录ssh localhost:
  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
b.修改配置文件：
etc/hadoop/core-site.xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
--------------------------------------------
etc/hadoop/hdfs-site.xml
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
--------------------------------------------
etc/hadoop/mapred-site.xml
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
    <description>The runtime framework for executing MapReduce jobs.
      Can be one of local, classic or yarn.
    </description>
  </property>
</configuration>
--------------------------------------------
etc/hadoop/yarn-site.xml
<configuration>
<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>

4.启动
格式化HDFS文件系统:
$ hdfs namenode -format
会看到/tmp/hadoop-$user/dfs文件夹，如果格式化有问题删掉这个文件夹再执行一次命令。
启动守护进程，sbin下：
$ ./start-dfs.sh
$ ./start-yarn.sh
会启动三个：一个namenode，一个secondary namenode，一个datanode，log在logs/下，看看有没有错误。
创建hdfs目录:
$ hdfs dfs -mkdir /user
$ hdfs dfs -mkdir /user/ryan
http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html
